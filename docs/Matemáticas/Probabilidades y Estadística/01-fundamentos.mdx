import Image from "@site/src/components/Image";
import MDXDetails from "@site/src/components/MDXDetails";

# Fundamentos de los modelos de probabilidad

## Álgebra de eventos

### Definiciones importantes

Consideremos algunas definiciones relacionadas a un fenómeno aleatorio:

- **Espacio muestral:** Conjunto de todos los resultados posibles.
- **Punto muestral:** Un resultado particular.
- **Evento:** Subconjunto de resultados posibles.

El espacio muestral puede ser discreto o continuo. El caso discreto
corresponde a un espacio muestral compuesto por un conjunto contable (numerable) de puntos muestrales, mientras que el caso continuo corresponde a un espacio muestral compuesto de un continuo
de puntos muestrales.

- **Evento Imposible:** Denotado por $\phi$ es un evento sin puntos muestrales.

- **Evento Certeza:** Denotado por $S$ u $\Omega$, es un evento que contiene a todos los puntos muestrales.

- **Evento Complemento:** Denotado por $\bar{E}$, contiene todos los puntos muestrales de $S$ que no están contenidos en un evento $E$.

- **Unión de Eventos:** Para dos eventos $E_1$ y $E_2$, su union forma un nuevo evento que contiene los puntos muestrales de $E_1$ y los contenidos en $E_2$ que no se encuentran en $E_1$.

- **Intersección de Eventos:** Para dos eventos $E_1$ y $E_2$, su intersección forma un nuevo evento que contiene los puntos muestrales contenidos en $E_1$ y en $E_2$ a la vez.

- **Eventos Mutuamente Excluyentes (Disjuntos):** Son eventos que no tienen puntos muestrales en común, es decir, su intersección es vacía.

- **Eventos Colectivamente Exhaustivos:** Son eventos que unidos conforman el espacio muestral.

$$
\begin{align*}
U: & \text{ Unión} \\
\cap: & \text{ Intersección} \\
\bar{E}: & \text{ Complemento de } E \\
\end{align*}
$$

### Operaciones matemáticas de conjuntos

- **Igualdad de Conjuntos:** Dos conjuntos son iguales si y sólo si ambos conjuntos contienen exactamente los mismos puntos muestrales. Un caso básico es el siguiente

  $$
  A \cup \phi=A
  $$

  donde $\phi$ representa un conjunto vacío.
  También se tiene que

  $$
  A \cap \phi=\phi
  $$

  Por lo tanto

  $$
  A \cup A=A \quad \text { y } A \cap A=A
  $$

  Con respecto al espacio muestral $S$

  $$
  A \cup S=S \quad \text { y } A \cap S=A
  $$

- **Conjunto complemento:** Con respecto a un evento $E$ y su complemento $\bar{E}$, se observa que

  $$
  E \cup \bar{E}=S \quad \text { y } \quad E \cap \bar{E}=\phi
  $$

  Finalmente

  $$
  \overline{\bar{E}}=E
  $$

- **Ley Conmutativa:** La unión e intersección de conjuntos son conmutativas, es decir, para dos conjuntos $A$ y $B$ se cumple que

  $$
  \begin{aligned}
  & A \cup B=B \cup A \\
  & A \cap B=B \cap A
  \end{aligned}
  $$

- **Ley Asociativa:** La unión e intersección de conjuntos es asociativa, es decir, para tres conjuntos $A, B$ y $C$ se cumple que

  $$
  \begin{aligned}
  & (A \cup B) \cup C=A \cup(B \cup C)=B \cup(A \cup C) \\
  & (A \cap B) \cap C=A \cap(B \cap C)=B \cap(A \cap C)
  \end{aligned}
  $$

- **Ley Distributiva:** La unión e intersección de conjuntos es distributiva, es decir, para tres conjuntos $A, B$ y $C$ se cumple que

  $$
  \begin{aligned}
  & (A \cup B) \cap C=(A \cap C) \cup(B \cap C) \\
  & (A \cap B) \cup C=(A \cup C) \cap(B \cup C)
  \end{aligned}
  $$

- **Ley de De Morgan:** Esta ley relaciona conjuntos y sus complementos.

  Para dos conjuntos (eventos), $E_1$ y $E_2$, la ley de De Morgan dice que

  $$
  \overline{\left(E_1 \cup E_2\right)}=\bar{E}_1 \cap \bar{E}_2 \quad \text { y } \overline{\left(E_1 \cap E_2\right)}=\bar{E}_1 \cup \bar{E}_2
  $$

  Generalizando

  $$
  \overline{\left(E_1 \cup E_2 \cup \cdots \cup E_n\right)}=\bar{E}_1 \cap \bar{E}_2 \cap \cdots \cap \bar{E}_n
  $$

  y

  $$
  \overline{\left(E_1 \cap E_2 \cap \cdots \cap E_n\right)}=\bar{E}_1 \cup \bar{E}_2 \cup \cdots \cup \bar{E}_n
  $$

### Axiomas fundamentales

Los axiomas son los siguientes:

- **Axioma 1:** Para cada evento $E$ contenido en un espacio muestral $S$ se tiene que

  $$
  P(E) \geq 0
  $$

- **Axioma 2:** La probabilidad del evento certeza $S$ es

  $$
  P(S)=1
  $$

- **Axioma 3:** Para dos eventos $E_1$ y $E_2$ mutuamente excluyentes (disjuntos),
  $$
  P\left(E_1 \cup E_2\right)=P\left(E_1\right)+P\left(E_2\right)
  $$

### Ley aditiva

Sea un evento $E$ y su complemento $\bar{E}$. Por ser eventos disjuntos se tiene que

$$
P(E \cup \bar{E})=P(E)+P(\bar{E})
$$

Además como $(E \cup \bar{E})=S$, se tiene que

$$
P(\bar{E})=1-P(E)
$$

Por otra parte

$$
P(E \cap \bar{E})=P(\phi)=0
$$

Finalmente para dos eventos cualquiera $E_1$ y $E_2$ la ley aditiva dice que

$$
P\left(E_1 \cup E_2\right)=P\left(E_1\right)+P\left(E_2\right)-P\left(E_1 \cap E_2\right)
$$

Esta ecuación aplicada a la unión de tres eventos $E_1$, $E_2$ y $E_3$ es

$$
\begin{aligned}
P\left(E_1 \cup E_2 \cup E_3\right)= & P\left[\left(E_1 \cup E_2\right) \cup E_3\right] \\
= & P\left(E_1 \cup E_2\right)+P\left(E_3\right)-P\left[\left(E_1 \cup E_2\right) \cap E_3\right] \\
= & P\left(E_1\right)+P\left(E_2\right)-P\left(E_1 \cap E_2\right)+P\left(E_3\right)-P\left[\left(E_1 \cap E_3\right) \cup\left(E_2 \cap E_3\right)\right] \\
= & P\left(E_1\right)+P\left(E_2\right)+P\left(E_3\right)-P\left(E_1 \cap E_2\right)-P\left(E_1 \cap E_3\right)-P\left(E_2 \cap E_3\right) \\
& +P\left(E_1 \cap E_2 \cap E_3\right)
\end{aligned}
$$

Para $n$ eventos cualquiera, por De Morgan se tiene lo siguiente:

$$
\begin{aligned}
P\left(E_1 \cup E_2 \cup \cdots \cup E_n\right) & =1-P\left(\overline{E_1 \cup E_2 \cup \cdots \cup E_n}\right) \\
& =1-P\left(\bar{E}_1 \cap \bar{E}_2 \cap \cdots \cap \bar{E}_n\right)
\end{aligned}
$$

En el caso de $E_1, \ldots, E_n$ sean eventos mutuamente excluyentes

$$
P\left(E_1 \cup E_2 \cup \cdots \cup E_n\right)=\sum_{i=1}^n P\left(E_i\right)
$$

## Métodos de conteo

Cuando los espacios muestrales son finitos, basta con asignar probabilidades a cada uno de los resultados posibles para luego obtener las probabilidad de un suceso simplemente sumando las probabilidades de ocurrencia de cada resultado básico que lo componen.

$$
S=\left\{\omega_1, \ldots, \omega_N\right\}
$$

con $p_i=P\left(\left\{\omega_i\right\}\right), i=1, \ldots, N$.
Para el caso de Probabilidad Clásica se tiene que para un suceso $A$ :

$$
P(A)=\frac{\# A}{\# S} = \frac{\text{Número de casos favorables}}{\text{Número de casos posibles}}
$$

### Principio de multiplicación

Si un experimento está compuesto de $k$ experimentos con tamaños muestrales $n_1, \ldots, n_k$, entonces

$$
\# S=n_1 \times n_2 \times \cdots \times n_k
$$

Por ejemplo, si se tienen $n_1$ maneras de realizar el primer experimento, $n_2$ maneras de realizar el segundo experimento, y así sucesivamente, entonces el número total de maneras de realizar el experimento compuesto es $n_1 \times n_2 \times \cdots \times n_k$.

### Permutación

Consideremos un conjunto de objetos

$$
C=\left\{c_1, \ldots, c_n\right\}
$$

y queremos seleccionar una muestra de $r$ objetos. ¿De cuántas maneras lo podemos hacer?

- Muestreo Con Reemplazo: $n^r$.
- Muestreo Sin Reemplazo: $n \times(n-1) \times(n-2) \times \cdots \times(n-r+1)$.

La permutación se denota como $P(n, r)$ y se define como

$$
P(n, r)=n \times(n-1) \times(n-2) \times \cdots \times(n-r+1)=\frac{n!}{(n-r)!}
$$

### Combinación

Consideremos un Muestreo Sin Reemplazo. Si nos interesa una muestra sin importar el orden de ingreso, la cantidad de muestras distintas de tamaño $r$ son

$$
\left(\begin{array}{l}
n \\
r
\end{array}\right)=\frac{n !}{r ! \times(n-r) !}
$$

Estos "números" se conocen como coeficientes binomiales y tienen la siguiente propiedad

$$
(a+b)^n=\sum_{k=0}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) a^k b^{n-k}
$$

### Ordenamiento multinomial

Queremos asignar $n$ objetos a $k$ grupos distintos de tamaños $n_1, \ldots$, $n_k$, con $\displaystyle\sum_{i=1}^k n_i=n$. El número de grupos distintos con las características dadas son

$$
\left(\begin{array}{c}
n \\
n_1 n_2 \cdots n_k
\end{array}\right)=\frac{n !}{n_{1} ! \times \cdots \times n_{k} !}
$$

Estos "números" se conocen como ordenamientos multinomiales y tienen la siguiente propiedad

$$
\left(x_1+\cdots+x_k\right)^n=\sum_{n_1=0}^n \sum_{n_2=0}^{n-n_1} \cdots \sum_{n_k=0}^{n-n_1-\cdots-n_{k-1}} \frac{n !}{n_{1} ! \times \cdots \times n_{k} !} x_1^{n_1} \times \cdots \times x_k^{n_k}
$$

## Probabilidad condicional

Cuando la ocurrencia de un evento (o no ocurrencia) depende de otro evento, es relevante ver la probabilidad como una probabilidad condicional.

Se define la probabilidad que un evento $E_1$ ocurra bajo el supuesto que otro evento $E_2$ ocurre con certeza a

$$
P\left(E_1 \mid E_2\right)=\frac{P\left(E_1 \cap E_2\right)}{P\left(E_2\right)}
$$

En general, la probabilidad de un evento $E$ ya está condicionada se condiciona a la ocurrencia del evento certeza $S$:

$$
P(E \mid S)=\frac{P(E \cap S)}{P(S)}=P(E)
$$

Consideremos las probabilidades de un evento $E_1$ y su complemento $\bar{E}_1$ condicionados a la ocurrencia previa de un evento $E_2$.

$$
P\left(E_1 \mid E_2\right)=\frac{P\left(E_1 \cap E_2\right)}{P\left(E_2\right)} \quad \text { y } \quad P\left(\bar{E}_1 \mid E_2\right)=\frac{P\left(\bar{E}_1 \cap E_2\right)}{P\left(E_2\right)}
$$

Si las sumamos tenemos que

$$
P\left(\bar{E}_1 \mid E_2\right)=1-P\left(E_1 \mid E_2\right)
$$

### Independencia estadística

Dos eventos $E_1$ y $E_2$ se dice que son estadísticamente independientes si la ocurrencia de un evento no depende de la ocurrencia o no ocurrencia del otro.

Es decir,

$$
P\left(E_1 \mid E_2\right)=P\left(E_1\right) \text { ó } P\left(E_2 \mid E_1\right)=P\left(E_2\right)
$$

A partir de la ecuación de probabilidad condicional se deduce que si $E_1$ y $E_2$ son eventos posibles entonces

$$
P\left(E_1 \cap E_2\right)=P\left(E_1 \mid E_2\right) \cdot P\left(E_2\right) \quad \text { ó } \quad P\left(E_1 \cap E_2\right)=P\left(E_2 \mid E_1\right) \cdot P\left(E_1\right)
$$

Si $E_1$ y $E_2$ fuesen eventos estadísticamente independientes entonces

$$
P\left(E_1 \cap E_2\right)=P\left(E_1\right) \cdot P\left(E_2\right)
$$

## Ley multiplicativa

Para tres eventos $E_1, E_2$ y $E_3$ la ley multiplicativa implica por ejemplo que

$$
P\left(E_1 \cap E_2 \cap E_3\right)=\left\{\begin{array}{l}
P\left(E_3 \mid E_1 \cap E_2\right) \cdot P\left(E_2 \mid E_1\right) \cdot P\left(E_1\right) \\
P\left(E_1 \cap E_2 \mid E_3\right) \cdot P\left(E_3\right)
\end{array}\right.
$$

### Independencia

Consideremos ahora los eventos $E_1, E_2, \ldots, E_n$. Estos eventos se dicen mutuamente independientes si y solo si, cualquier sub-colección de eventos de ellos $E_{i 1}, E_{i 2}, \ldots, E_{i m}$ cumple con la siguiente condición

$$
P\left(E_{i 1} \cap E_{i 2} \cap \cdots \cap E_{i m}\right)=P\left(E_{i 1}\right) \times P\left(E_{i 2}\right) \times \cdots \times P\left(E_{i m}\right)
$$

### Propiedades

- Si $E_1$ y $E_2$ son eventos estadísticamente independientes, entonces $\bar{E}_1$ y $\bar{E}_2$ también lo son.
- Si $E_1$ y $E_2$ son eventos estadísticamente independientes dado un evento $A$, entonces
  $$
  P\left(E_1 \cap E_2 \mid A\right)=P\left(E_1 \mid A\right) \cdot P\left(E_2 \mid A\right)
  $$
- Para dos eventos cualesquiera $E_1$ y $E_2$ se tiene que
  $$
  P\left(E_1 \cup E_2 \mid A\right)=P\left(E_1 \mid A\right)+P\left(E_2 \mid A\right)-P\left(E_1 \cap E_2 \mid A\right)
  $$

## Teorema de probabilidades totales

Considere $n$ eventos posibles $E_1, E_2, \ldots, E_n$ colectivamente exhaustivos y mutuamente excluyentes, es decir,

$$
\bigcup_{i=1}^n E_i=S \quad \text { y } \quad E_i \cap E_j=\phi \quad \forall i \neq j
$$

Entonces

$$
A=A \cap S=A \cap\left[\bigcup_{i=1}^n E_i\right]=\bigcup_{i=1}^n\left(A \cap E_i\right),
$$

con $\left(A \cap E_1\right), \ldots,\left(A \cap E_n\right)$ eventos mutuamente excluyentes.
Por lo tanto, por axioma 3 y ley multiplcativa

$$
P(A)=\sum_{i=1}^n P\left(A \cap E_i\right)=\sum_{i=1}^n P\left(A \mid E_i\right) \cdot P\left(E_i\right)
$$

## Teorema de Bayes

Si cada evento $E_j$ de la partición de $S$ y el evento $A$ son posibles, entonces por la ley multiplicativa se tiene que

$$
P\left(A \mid E_j\right) \cdot P\left(E_j\right)=P\left(E_j \mid A\right) \cdot P(A)
$$

Es decir,

$$
P\left(E_j \mid A\right)=\frac{P\left(A \mid E_j\right) \cdot P\left(E_j\right)}{P(A)}
$$

Aplicando el teorema de probabilidades totales se tiene que

$$
P\left(E_j \mid A\right)=\frac{P\left(A \mid E_j\right) \cdot P\left(E_j\right)}{\sum_{i=1}^n P\left(A \mid E_i\right) \cdot P\left(E_i\right)} = \frac{P\left(A \mid E_j\right) \cdot P\left(E_j\right)}{P(A)}
$$

Este resultado se conoce como el Teorema de Bayes. En general, una fórmula del teorema de Bayes para dos eventos $A$ y $B$ es

$$
P\left(A \mid B\right)=\frac{P\left(B \mid A\right) \cdot P(A)}{P(B)}
$$

<Image
  src="matematicas/probabilidades_estadistica/fundamentos/tree_diagram.png"
  alt="Diagrama de árbol"
  caption="Diagrama de árbol para dos eventos A y B"
  width="35%"
/>